# Práctica de PySpark - DataFrames API en Databricks

![dataframes-api](https://github.com/user-attachments/assets/fc115908-7d93-4d18-a3ee-4bfff5615eea)


Este proyecto tiene como objetivo enseñar el uso de diversas herramientas y comandos en PySpark, específicamente en la **DataFrames API**. A través de una serie de ejercicios prácticos, aprenderás a realizar operaciones comunes y avanzadas sobre grandes volúmenes de datos en entornos de Apache Spark. 

## **Descripción del Proyecto**

En esta práctica, exploraremos cómo realizar las siguientes tareas clave utilizando PySpark:

- **Filtrado de datos**: Selección de datos según condiciones específicas.
- **Agregaciones**: Cálculos sobre los datos agrupados, como sumas, promedios, y conteos.
- **Transformaciones de datos**: Uso de funciones para limpiar y transformar cadenas (por ejemplo, `ltrim`, `rtrim`, `trim`).
- **Manejo de datos JSON**: Cómo trabajar con datos en formato JSON dentro de PySpark.
- **Joins (Uniones)**: Combinación de datos de múltiples DataFrames mediante SQL y Python.
- **UDFs (Funciones Definidas por el Usuario)**: Creación de funciones personalizadas para aplicar a los datos.
- **Pandas y DataFrames**: Uso de Pandas para manipulación de datos y su interoperabilidad con PySpark DataFrames.
- **Estadísticas descriptivas**: Cálculo de métricas como **skewness**, **kurtosis**, **variance**, **stddev**, entre otras.
- **Actividades prácticas.**

Además, se incluyen ejemplos visuales y soluciones completas para facilitar la comprensión de cada concepto y permitir que los usuarios puedan practicar y experimentar.

## **Estructura del Proyecto**

Este repositorio está organizado de la siguiente manera:

- **`/dataset`**: Carpeta que contiene los archivos de datos necesarios para realizar los ejercicios de la práctica. Los archivos están organizados en colecciones y se utilizan para los ejemplos de operaciones como filtrado, agregación, y transformación.
  
- **`/images`**: Carpeta destinada a guardar las imágenes generadas durante la práctica, como capturas de tablas, gráficos y otros outputs visuales. Estas imágenes se corresponden a los ejemplos mostrados en el notebook.

- **`/solutions`**: Carpeta donde se almacenan las soluciones completas a los ejercicios propuestos. Esta carpeta es útil como referencia para quienes necesiten comprobar sus resultados o necesiten ayuda.

- **`notebook.ipynb`**: Notebook vacío en la raíz del proyecto que contiene las instrucciones y pasos para realizar la práctica. Aquí es donde deberás realizar todas las actividades desde el principio.

## **Pasos Previos**

Antes de comenzar con esta práctica en **Databricks**, sigue estos pasos:

1. **Crear una cuenta en Databricks**: Si aún no tienes una cuenta, puedes crear una cuenta gratuita en Databricks. Este entorno te permitirá trabajar de manera eficiente con PySpark y procesar grandes volúmenes de datos en la nube.
  
2. **Subir los Archivos de Datos**: Asegúrate de cargar los archivos de datos que se encuentran en la carpeta `/dataset` a tu espacio de trabajo en Databricks. Estos archivos son necesarios para los ejercicios prácticos.

3. **Configurar el Entorno**: Una vez en Databricks, crea un clúster de Spark para ejecutar el código de PySpark. Esto es fundamental para realizar las tareas de procesamiento distribuido que ofrece Spark.

4. **Abrir el Notebook**: Abre el archivo `notebook_practice.ipynb` en Databricks. Este notebook contiene todos los pasos que deberás seguir para completar la práctica y resolver los ejercicios.

## **Repositorio**

Puedes descargar el repositorio completo y acceder a todos los archivos necesarios desde el siguiente enlace:

[Repositorio de la práctica en GitHub](https://github.com/leo-narvaez/dataframes-api)

Dentro de este repositorio encontrarás:
- Los archivos de datos en la carpeta `/dataset`.
- El notebook con las instrucciones para realizar la práctica.
- Las soluciones a los ejercicios en la carpeta `/solutions`.

## **Objetivos de Aprendizaje**

Al completar esta práctica, habrás aprendido a:

- Trabajar con grandes volúmenes de datos en Apache Spark usando PySpark.
- Realizar agregaciones, transformaciones y uniones entre diferentes conjuntos de datos.
- Utilizar funciones avanzadas de PySpark para estadísticas descriptivas.
- Trabajar con datos en formato JSON y aplicar técnicas de limpieza y manipulación de cadenas.
- Interoperar con Pandas y realizar análisis de datos de forma eficiente.

## **Conclusión**

Este proyecto te proporciona una excelente base para comenzar a trabajar con PySpark y aprender cómo manipular y analizar datos a gran escala utilizando la poderosa DataFrame API de PySpark. Aprovecha los ejemplos y las soluciones para practicar y mejorar tus habilidades de análisis de datos en la nube con Databricks.

---

¡No dudes en abrir un **issue** si tienes alguna pregunta o sugerencia!

